{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "feb1ba62-94e1-4ee0-b0f3-8ebcf470b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Mapping\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from selenium.webdriver.firefox.webdriver import WebDriver\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "23fbb8b1-c8c4-497f-8994-c4d2c47d2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_title_selector = \"h1.entry-title\"\n",
    "story_author_selector = \"span.post-author\"\n",
    "story_section_selector = \"a.post-category\"\n",
    "\n",
    "\"\"\" Extracts metadata from the story at `url`\n",
    "Args:\n",
    "    driver: The Selenium Firefox WebDriver instance\n",
    "    url: A url that hopefully points to a story\n",
    "Returns:\n",
    "    A dictionary containing metadata on the story. You'll probably want to capture more metadata than I did. \n",
    "    I imagine we might want the full text of the story, etc, etc.\n",
    "\"\"\"\n",
    "\n",
    "def scrape_story(driver: WebDriver, url: str) -> Mapping[str, Any]:\n",
    "    print(\"scraping story\", url)\n",
    "    open_url_in_new_tab(driver, url)  # Open the story in a new tab\n",
    "    \n",
    "    story_title = driver.find_element(By.CSS_SELECTOR, story_title_selector).text  # Since we want raw metadata, we're accessing the elements' text attributes\n",
    "    story_author = driver.find_element(By.CSS_SELECTOR, story_author_selector).text\n",
    "    story_section = driver.find_element(By.CSS_SELECTOR, story_section_selector).text\n",
    "    story_text = \"\" #...\n",
    "    \n",
    "    # ...\n",
    "    # capture additional metadata\n",
    "    # ...\n",
    "    story_metadata = {\n",
    "        \"title\": story_title,\n",
    "        \"author\": story_author,\n",
    "        \"section\": story_section,\n",
    "        \"url\": url,\n",
    "        #... additional metadata\n",
    "    }\n",
    "    close_current_tab(driver)\n",
    "    return story_metadata, driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bee42116-7b4f-452c-961f-fa7954cf2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Closes the current selenium driver tab and switches context to the first open tab\n",
    "Args:\n",
    "    driver: The Selenium Firefox WebDriver instance\n",
    "Returns:\n",
    "    None\n",
    "\"\"\"\n",
    "def close_current_tab(driver: WebDriver) -> None:\n",
    "    driver.close()\n",
    "    main_tab_handle = driver.window_handles[0]\n",
    "    driver.switch_to.window(main_tab_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f1b8e52-bf8d-493a-ad98-2eb51b185893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Opens a new selenium driver  tab and navigates to the url provided\n",
    "Args:\n",
    "    driver: The Selenium Firefox WebDriver instance\n",
    "    url: The url to navigate to\n",
    "Returns:\n",
    "    None\n",
    "\"\"\"\n",
    "def open_url_in_new_tab(driver: WebDriver, url: str) -> None:\n",
    "    driver.switch_to.new_window() # opens a new tab\n",
    "    new_tab_handle = driver.window_handles[-1]  # grab the last browser handle (which we just opened)\n",
    "    driver.switch_to.window(new_tab_handle)\n",
    "    driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "566b1889-a46c-4cdd-beed-2146736b2c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 16.9kB [00:00, 13.9MB/s]                                                                                                                                                                        \n"
     ]
    }
   ],
   "source": [
    "##### Load landing page for individual Courier Site\n",
    "\n",
    "driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install()))  # Ensure we have the latest GeckoDriver (used to run Firefox)\n",
    "landing_page_url = \"https://cardinalpine.com/\"  # The landing page (hopefully) contains a dynamic list of all the stories we need to scrape from that outlet \n",
    "driver.get(landing_page_url)  # Navigate to the landing page  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40d24904-1467-4cb6-9062-ea5f5bf07345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Find the \"recent stories\" feed\n",
    "\n",
    "feed_container_class = \"recent-posts\"  # This is the HTML class of the div containing the story feed \n",
    "feed_container_element = driver.find_element(By.CLASS_NAME, feed_container_class)  # We'll capture that div in a python variable for easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93b6c51e-d760-4734-b2de-7dc246693361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 stories in feed\n",
      "scraping story https://cardinalpine.com/story/celebrate-pi-day-in-north-carolina-with-discount-pizza-and-philosophy/\n",
      "scraping story https://cardinalpine.com/story/nc-supreme-court-will-hear-voting-rights-cases-it-already-decided/\n",
      "scraping story https://cardinalpine.com/story/women-workers-here-are-tools-to-get-the-pay-you-deserve-copy/\n",
      "scraping story https://cardinalpine.com/story/survey-give-us-your-oscar-picks/\n",
      "scraping story https://cardinalpine.com/story/biden-tax-billionaires-corporations-strengthen-medicare-child-care-help-families-strong/\n",
      "scraping story https://cardinalpine.com/story/north-carolina-lawmakers-rep-tim-longest-wake-county/\n",
      "scraping story https://cardinalpine.com/story/15-highest-rated-restaurants-for-a-night-out-on-the-town-in-greenville/\n",
      "scraping story https://cardinalpine.com/story/the-tragedy-of-north-carolinas-leandro-case/\n"
     ]
    }
   ],
   "source": [
    "#### Now it's time to begin actual scraping. If you press the \"LOAD MORE STORIES\" link at the bottom of the feed\n",
    "#### You'll find that the page will load 8 additional stories\n",
    "#### Given that behavior, we should set up a loop that will iteratively scrape stories, load more stories, scrape stories, load more stories... and so on\n",
    "\n",
    "\n",
    "# We'll use \"number_of_stories_seen\" to form our loop condition.  \n",
    "# If we press \"load more stories\" but the number of stories in the feed doesn't increment above \"number_of_stories_seen, we're presumably done. \n",
    "# I say presumably, because I'm not sure how long we can keep loading stories without breaking the page\n",
    "# And even if we can keep loading for a while, I'm not sure doing so wil capture every story Cardinal & Pine has published\n",
    "# You'll want to do some sanity checks to ensure this approach really can capture all the stories. \n",
    "\n",
    "# If it happens to work, great! If not, we'll have to devise something new :)\n",
    "# Feel free to reach out if you'd like to talk strategy. For now, take this script as more of a scraping tutorial than a perfect methodology for logging all stories\n",
    "# Some of the \"fun\" of scraping comes from figuring out how to get the content you want\n",
    "\n",
    "number_of_stories_seen = 0  # And without further ado, the variable in question \n",
    "\n",
    "# This is a CSS selector. # See https://saucelabs.com/resources/blog/selenium-tips-css-selectors for more info\n",
    "# Each story element in the feed is contained in a div with the class \"item\"\n",
    "# We can select it with this CSS selector syntax\n",
    "story_container_css_selector = \"div.item\" \n",
    "\n",
    "story_url_css_selector = \"a.item-title\" # Another CSS selector that we'll use to find story URLs \n",
    "\n",
    "all_stories_metadata = []\n",
    "\n",
    "# I dislike the \"while True\" syntax, but it's standard in Python\n",
    "# See https://peps.python.org/pep-0315/ and https://twitter.com/raymondh/status/1528772337306419200\n",
    "# We'll break the loop when we reach our exit condition\n",
    "while True:\n",
    "    \n",
    "    # Capture all of the story container divs in our feed\n",
    "    # Note that we're using \"find_elements\" here, not \"find_element\" like we used above. \n",
    "    # find_elements will return all of the elements matching our selection criterion. find_element only returns the first\n",
    "    story_container_elements = feed_container_element.find_elements(By.CSS_SELECTOR, story_container_css_selector)  \n",
    "    \n",
    "    # If we've seen all the stories in the feed, we're done. Break the loop\n",
    "    number_of_stories_in_feed = len(story_container_elements)\n",
    "    if number_of_stories_in_feed == number_of_stories_seen:\n",
    "        print(f\"No more stories in feed. Processed {number_of_stories_seen} stories total\")\n",
    "        break \n",
    "    \n",
    "    print(f\"{number_of_stories_in_feed} stories in feed\")\n",
    "    # Stories load 8 at a time. We should always scrape the most recently loaded 8 stories. \n",
    "    for story_container_element in story_container_elements[-8:]:\n",
    "        # Find the \"a\" tag containing the story link\n",
    "        # Then extract its \"href\" attribute, AKA the story's URL\n",
    "        story_url = story_container_element.find_element(By.CSS_SELECTOR, story_url_css_selector).get_attribute(\"href\")\n",
    "        \n",
    "        # Do the actual scraping\n",
    "        story_metadata = scrape_story(driver, story_url)  \n",
    "        \n",
    "        # Store the current story's metadata in our larger data structure\n",
    "        all_stories_metadata.append(story_metadata) \n",
    "        \n",
    "        # increment our stories seen counter\n",
    "        number_of_stories_seen += 1\n",
    "    \n",
    "    # After each set of 8 stories, we'll need to load more. \n",
    "    # Remove the break below and find a way to click on the \"load more stories\" link\n",
    "    # You may need to make sure the link is visible before you click it (scroll to it)\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "520a71d0-6b0e-4f51-b07a-fa59288fc4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### eventually you'll want to write that all_stories_metadata array to a database\n",
    "### when you're designing a schema for the database, you might consider creating one table for outlets, one for stories, and another for authors\n",
    "### although if you have a solution you like better, feel free to go that way.  \n",
    "### And again, email or drop a message if you have questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca25d564-9e26-4023-952f-f70205a09400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
